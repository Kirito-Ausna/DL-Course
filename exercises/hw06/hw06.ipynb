{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6374a79",
   "metadata": {},
   "source": [
    "## Homework Assignment 6 - Optimizers in deep learning\n",
    "\n",
    "Try the following optimizers:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75ce1e9b",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "In this section we are going to introduce the basic concepts underlying *gradient descent*.\n",
    "Although it is rarely used directly in deep learning, an understanding of gradient descent is key to understanding stochastic gradient descent algorithms.\n",
    "For instance, the optimization problem might diverge due to an overly large learning rate. This phenomenon can already be seen in gradient descent. Likewise, preconditioning is a common technique in gradient descent and carries over to more advanced algorithms.\n",
    "Let's start with a simple special case.\n",
    "\n",
    "\n",
    "## One-Dimensional Gradient Descent\n",
    "\n",
    "Gradient descent in one dimension is an excellent example to explain why the gradient descent algorithm may reduce the value of the objective function. Consider some continuously differentiable real-valued function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$. Using a Taylor expansion we obtain\n",
    "\n",
    "$$f(x + \\epsilon) = f(x) + \\epsilon f'(x) + \\mathcal{O}(\\epsilon^2).$$\n",
    "\n",
    "That is, in first-order approximation $f(x+\\epsilon)$ is given by the function value $f(x)$ and the first derivative $f'(x)$ at $x$. It is not unreasonable to assume that for small $\\epsilon$ moving in the direction of the negative gradient will decrease $f$. To keep things simple we pick a fixed step size $\\eta > 0$ and choose $\\epsilon = -\\eta f'(x)$. Plugging this into the Taylor expansion above we get\n",
    "\n",
    "$$f(x - \\eta f'(x)) = f(x) - \\eta f'^2(x) + \\mathcal{O}(\\eta^2 f'^2(x)).$$\n",
    "\n",
    "If the derivative $f'(x) \\neq 0$ does not vanish we make progress since $\\eta f'^2(x)>0$. Moreover, we can always choose $\\eta$ small enough for the higher-order terms to become irrelevant. Hence we arrive at\n",
    "\n",
    "$$f(x - \\eta f'(x)) \\lessapprox f(x).$$\n",
    "\n",
    "This means that, if we use\n",
    "\n",
    "$$x \\leftarrow x - \\eta f'(x)$$\n",
    "\n",
    "to iterate $x$, the value of function $f(x)$ might decline. Therefore, in gradient descent we first choose an initial value $x$ and a constant $\\eta > 0$ and then use them to continuously iterate $x$ until the stop condition is reached, for example, when the magnitude of the gradient $|f'(x)|$ is small enough or the number of iterations has reached a certain value.\n",
    "\n",
    "For simplicity we choose the objective function $f(x)=x^2$ to illustrate how to implement gradient descent. Although we know that $x=0$ is the solution to minimize $f(x)$, we still use this simple function to observe how $x$ changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "256754c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a006dac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):  # Objective function\n",
    "    return x ** 2\n",
    "\n",
    "def f_grad(x):  # Gradient (derivative) of the objective function\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62c59cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(eta, f_grad):\n",
    "    x = 10.0 # set initial value\n",
    "\n",
    "    # Tips: try to do gd update for 10 epoches\n",
    "    pass\n",
    "    \n",
    "    return results\n",
    "\n",
    "results = gd(0.2, f_grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c6e27f0",
   "metadata": {},
   "source": [
    "The progress of optimizing over $x$ can be plotted as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce950f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# util function to visualize optimization process\n",
    "def plot(X, Y=None, xlabel=None, ylabel=None, legend=[], xlim=None,\n",
    "         ylim=None, xscale='linear', yscale='linear',\n",
    "         fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):\n",
    "    \"\"\"Plot data points.\"\"\"\n",
    "\n",
    "    def has_one_axis(X):  # True if `X` (tensor or list) has 1 axis\n",
    "        return (hasattr(X, \"ndim\") and X.ndim == 1 or isinstance(X, list)\n",
    "                and not hasattr(X[0], \"__len__\"))\n",
    "\n",
    "    if has_one_axis(X): X = [X]\n",
    "    if Y is None:\n",
    "        X, Y = [[]] * len(X), X\n",
    "    elif has_one_axis(Y):\n",
    "        Y = [Y]\n",
    "    if len(X) != len(Y):\n",
    "        X = X * len(Y)\n",
    "\n",
    "    if axes is None: axes = plt.gca()\n",
    "    axes.cla()\n",
    "    for x, y, fmt in zip(X, Y, fmts):\n",
    "        axes.plot(x,y,fmt) if len(x) else axes.plot(y,fmt)\n",
    "\n",
    "def show_trace(results, f):\n",
    "    n = max(abs(min(results)), abs(max(results)))\n",
    "    f_line = torch.arange(-n, n, 0.01)\n",
    "    plot([f_line, results], [[f(x) for x in f_line], [\n",
    "        f(x) for x in results]], 'x', 'f(x)', fmts=['-', '-o'])\n",
    "    \n",
    "show_trace(results, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7945587d",
   "metadata": {},
   "source": [
    "### Learning Rate\n",
    "\n",
    "The learning rate $\\eta$ can be set by the algorithm designer. If we use a learning rate that is too small, it will cause $x$ to update very slowly, requiring more iterations to get a better solution. To show what happens in such a case, consider the progress in the same optimization problem for $\\eta = 0.05$. As we can see, even after 10 steps we are still very far from the optimal solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aeada08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tips: try to plot fig with lr = 0.05\n",
    "pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7349386e",
   "metadata": {},
   "source": [
    "Conversely, if we use an excessively high learning rate, $\\left|\\eta f'(x)\\right|$ might be too large for the first-order Taylor expansion formula. That is, the term $\\mathcal{O}(\\eta^2 f'^2(x))$ in :eqref:`gd-taylor-2` might become significant. In this case, we cannot guarantee that the iteration of $x$ will be able to lower the value of $f(x)$. For example, when we set the learning rate to $\\eta=1.1$, $x$ overshoots the optimal solution $x=0$ and gradually diverges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35918d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tips: try to plot fig with lr = 1.1\n",
    "pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6bd7576",
   "metadata": {},
   "source": [
    "### Local Minima\n",
    "\n",
    "To illustrate what happens for nonconvex functions consider the case of $f(x) = x \\cdot \\cos(cx)$ for some constant $c$. This function has infinitely many local minima. Depending on our choice of the learning rate and depending on how well conditioned the problem is, we may end up with one of many solutions. The example below illustrates how an (unrealistically) high learning rate will lead to a poor local minimum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0baa1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.tensor(0.15 * np.pi)\n",
    "\n",
    "def f(x):  # Tips: objective function\n",
    "    return x * torch.cos(c * x)\n",
    "\n",
    "def f_grad(x):  \n",
    "    # Tips: try to compute the gradient\n",
    "    pass\n",
    "\n",
    "# Tips: try to plot fig with learning rate = 2\n",
    "pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abe72de7",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "In earlier chapters we kept using stochastic gradient descent in our training procedure, however, without explaining why it works.\n",
    "To shed some light on it,\n",
    "we just described the basic principles of gradient descent.\n",
    "In this section, we go on to discuss\n",
    "*stochastic gradient descent* in greater detail.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3bea288e",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Updates\n",
    "\n",
    "In deep learning, the objective function is usually the average of the loss functions for each example in the training dataset.\n",
    "Given a training dataset of $n$ examples,\n",
    "we assume that $f_i(\\mathbf{x})$ is the loss function\n",
    "with respect to the training example of index $i$,\n",
    "where $\\mathbf{x}$ is the parameter vector.\n",
    "Then we arrive at the objective function\n",
    "\n",
    "$$f(\\mathbf{x}) = \\frac{1}{n} \\sum_{i = 1}^n f_i(\\mathbf{x}).$$\n",
    "\n",
    "The gradient of the objective function at $\\mathbf{x}$ is computed as\n",
    "\n",
    "$$\\nabla f(\\mathbf{x}) = \\frac{1}{n} \\sum_{i = 1}^n \\nabla f_i(\\mathbf{x}).$$\n",
    "\n",
    "If gradient descent is used, the computational cost for each independent variable iteration is $\\mathcal{O}(n)$, which grows linearly with $n$. Therefore, when the  training dataset is larger, the cost of gradient descent for each iteration will be higher.\n",
    "\n",
    "Stochastic gradient descent (SGD) reduces computational cost at each iteration. At each iteration of stochastic gradient descent, we uniformly sample an index $i\\in\\{1,\\ldots, n\\}$ for data examples at random, and compute the gradient $\\nabla f_i(\\mathbf{x})$ to update $\\mathbf{x}$:\n",
    "\n",
    "$$\\mathbf{x} \\leftarrow \\mathbf{x} - \\eta \\nabla f_i(\\mathbf{x}),$$\n",
    "\n",
    "where $\\eta$ is the learning rate. We can see that the computational cost for each iteration drops from $\\mathcal{O}(n)$ of the gradient descent to the constant $\\mathcal{O}(1)$. Moreover, we want to emphasize that the stochastic gradient $\\nabla f_i(\\mathbf{x})$ is an unbiased estimate of the full gradient $\\nabla f(\\mathbf{x})$ because\n",
    "\n",
    "$$\\mathbb{E}_i \\nabla f_i(\\mathbf{x}) = \\frac{1}{n} \\sum_{i = 1}^n \\nabla f_i(\\mathbf{x}) = \\nabla f(\\mathbf{x}).$$\n",
    "\n",
    "This means that, on average, the stochastic gradient is a good estimate of the gradient.\n",
    "\n",
    "Now, we will compare it with gradient descent by adding random noise with a mean of 0 and a variance of 1 to the gradient to simulate a stochastic gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ad3488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x1, x2):  # Objective function\n",
    "    return x1 ** 2 + 2 * x2 ** 2\n",
    "\n",
    "def f_grad(x1, x2):  \n",
    "    # Tips: try to compute the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cc75d895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(x1, x2, s1, s2, f_grad):\n",
    "    # `s1` and `s2` are internal state variables that will be used in Momentum, adagrad, RMSProp\n",
    "    # Tips: try to finish Stochastic Gradient Descent\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e79da63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_2d(trainer, steps=20, f_grad=None):\n",
    "    # Optimize a 2D objective function with a customized trainer.\n",
    "\n",
    "    # initial values\n",
    "    x1, x2, s1, s2 = -5, -2, 0, 0\n",
    "    \n",
    "    # Tips: try to train by using your sgd\n",
    "    pass\n",
    "\n",
    "def show_trace_2d(f, results):\n",
    "    # Show the trace of 2D variables during optimization.\n",
    "    # results is the update information during sgd training\n",
    "\n",
    "    plt.plot(*zip(*results), '-o', color='#ff7f0e')\n",
    "    x1, x2 = torch.meshgrid(torch.arange(-5.5, 1.0, 0.1),\n",
    "                            torch.arange(-3.0, 1.0, 0.1))\n",
    "    plt.contour(x1, x2, f(x1, x2), colors='#1f77b4')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03c666aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1\n",
    "lr = 1.0  # Constant learning rate\n",
    "\n",
    "# Tips: try to plot the training trace with steps=50"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
